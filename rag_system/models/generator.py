import torch
import asyncio
import time
import os
import threading
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from typing import List, Dict, Optional, AsyncGenerator, TypedDict
from config.settings import settings
from models.vectorizer import Vectorizer
from langgraph.graph import StateGraph, END


class RAGState(TypedDict):
    question: str
    documents: List[str]
    reformul√©: bool
    reformulation_count: int
    r√©ponse: Optional[str]


class Generator:
    def __init__(self, vectorizer: Vectorizer):
        self.vectorizer = vectorizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            settings.LLM_MODEL_NAME,
            trust_remote_code=True,
            token=settings.HF_TOKEN
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            settings.LLM_MODEL_NAME,
            device_map="auto",
            torch_dtype="auto",
            trust_remote_code=True
        )
        # Set tokenizer and model in vectorizer for search
        self.vectorizer.tokenizer = self.tokenizer
        self.vectorizer.model = self.model

        # Initialize the agent graph
        self.app = self._create_agent_graph()

    def _llm_generate(self, prompt: str, max_new_tokens=512, temperature=0.0, stream=False) -> str:
        """Generate response using the LLM"""
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        inputs = self.tokenizer(text, return_tensors="pt").to("cuda")

        if stream:
            streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
            generation_kwargs = dict(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=temperature > 0.0,
                streamer=streamer,
            )
            thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()

            output = "".join(list(streamer))
            thread.join()
            return output.strip()
        else:
            with torch.no_grad():
                out_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=temperature > 0.0
                )[0]

            input_length = inputs.input_ids.shape[1]
            response_ids = out_ids[input_length:]
            response = self.tokenizer.decode(response_ids, skip_special_tokens=True)
            return response.strip()

    def _create_agent_graph(self):
        """Create the agent graph with your RAG logic"""
        graph = StateGraph(RAGState)
        graph.add_node("agent_node", self.agent_node)
        graph.set_entry_point("agent_node")

        def decision_router(state):
            return "answer" if state.get("r√©ponse") else "terminate"

        graph.add_conditional_edges("agent_node", decision_router, {"answer": END, "terminate": END})
        return graph.compile()

    def tool_reformulate(self, question: str, docs: list[dict]) -> str:
        """Reformulate the question based on documents"""
        context = "\n".join(f"- {d['text'][:500]}..." for d in docs[:3])
        prompt = (
            "Tu es un agent de la fonction publique du S√©n√©gal. "
            "La question suivante n‚Äôa pas obtenu de r√©ponse suffisante. "
            "Voici les extraits les plus pertinents trouv√©s :\n"
            f"{context}\n\n"
            "Reformule la question pour cibler plus pr√©cis√©ment l'information manquante, sans jamais modifier le sens "
            "en t‚Äôinspirant du vocabulaire des extraits si pertinent.\n"
            f"Question originale : {question}\n"
            "Nouvelle question :"
        )
        return self._llm_generate(prompt, max_new_tokens=128, temperature=0.0)

    def tool_judge(self, question: str, docs: list[dict]) -> str:
        """Judge if documents are sufficient to answer"""
        if not docs:
            return "REFORMULER"

        context = "\n\n".join(f"- {d['text']}" for d in docs)
        prompt = f"""Sur une √©chelle de 1-10, √† quel point ces extraits permettent-ils de r√©pondre √† la question ? R√©ponds avec juste le chiffre.

Question: {question}

Extraits:
{context}

Score (1-10):"""

        score_text = self._llm_generate(prompt, max_new_tokens=3, temperature=0.0)
        try:
            score = int(score_text.strip())
            return "PERTINENT" if score >= 6 else "REFORMULER"
        except:
            return "REFORMULER"

    async def _stream_answer(self, question: str, docs: list[dict]) -> AsyncGenerator[str, None]:
        """Stream the answer generation token by token"""
        context = "\n\n".join(f"[{i+1}] {d['text']}" for i, d in enumerate(docs))
        prompt = (
            "R√©ponds √† la question en 5 phrases maximum.\n"
            "Inclue 1-2 citations EXACTES (entre guillemets) tir√©es des extraits.\n"
            "√Ä la fin, ajoute une ligne 'Sources: ...' listant fichier:page.\n\n"
            f"Question:\n{question}\n\nExtraits:\n{context}\n\nR√©ponse:"
        )

        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        inputs = self.tokenizer(text, return_tensors="pt").to("cuda")

        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
        generation_kwargs = dict(
            **inputs,
            max_new_tokens=512,
            temperature=0.1,
            do_sample=True,
            streamer=streamer,
        )
        thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        for new_text in streamer:
            yield new_text
            await asyncio.sleep(0)

        thread.join()

        # Ajouter les sources
        sources = [f"{os.path.basename(d['source'])}:p.{d['page']+1}" for d in docs]
        yield f"\n\nüìö Sources: " + ", ".join(sorted(set(sources)))

    async def generate_stream(self, question: str, max_tokens: int = 512, temperature: float = 0.1):
        """Generate response with streaming using the agent"""
        question_current = question

        for step in range(1, settings.MAX_STEPS + 1):
            print(f"üìö Recherche d'informations (tentative {step})...")
            docs = self.vectorizer.hybrid_search(question_current, top_k=settings.TOP_K)

            verdict = self.tool_judge(question_current, docs)
            if verdict == "PERTINENT":
                
                async for token in self._stream_answer(question_current, docs):
                    yield token
                return
            else:
                if step == settings.MAX_STEPS:
                    yield "\n‚ùå Aucun document suffisamment pertinent trouv√©.\n"
                    return

                
                print("üîÑ Reformulation de la question...")
                question_current = self.tool_reformulate(question_current, docs)
                print(f"üí° Nouvelle question : {question_current}")

    def generate_sync(self, question: str) -> str:
        """Synchronous generation using the agent"""
        initial_state = {
            "question": question,
            "documents": [],
            "reformul√©": False,
            "reformulation_count": 0,
            "r√©ponse": None
        }
        final_state = self.app.invoke(initial_state)
        return final_state.get("r√©ponse", "Aucune r√©ponse g√©n√©r√©e.")

    def agent_node(self, state):
        """Main agent node with your RAG logic"""
        question = state["question"]

        for step in range(1, settings.MAX_STEPS + 1):
            docs = self.vectorizer.hybrid_search(question, top_k=settings.TOP_K)
            verdict = self.tool_judge(question, docs)

            if verdict == "PERTINENT":
                answer = self._llm_generate(
                    f"R√©ponds bri√®vement √† la question : {question}",
                    max_new_tokens=256,
                    temperature=0.1,
                    stream=False
                )
                return {
                    "decision": "answer",
                    "documents": [d["text"] for d in docs],
                    "r√©ponse": answer,
                    "question": question
                }
            else:
                if step == settings.MAX_STEPS:
                    return {
                        "decision": "terminate",
                        "documents": [d["text"] for d in docs],
                        "r√©ponse": "Je n‚Äôai trouv√© aucun document suffisamment pertinent."
                    }
                question = self.tool_reformulate(question, docs)

        return {
            "decision": "terminate",
            "documents": [],
            "r√©ponse": "Arr√™t sans r√©ponse."
        }
