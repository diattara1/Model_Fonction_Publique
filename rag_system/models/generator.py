import torch
import asyncio
import os
import threading
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from typing import List, Dict, Optional, AsyncGenerator, TypedDict
from config.settings import settings
from models.vectorizer import Vectorizer
from langgraph.graph import StateGraph, END


class RAGState(TypedDict):
    question: str
    documents: List[str]
    reformul√©: bool
    reformulation_count: int
    r√©ponse: Optional[str]


class Generator:
    def __init__(self, vectorizer: Vectorizer):
        self.vectorizer = vectorizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            settings.LLM_MODEL_NAME,
            trust_remote_code=True,
            token=settings.HF_TOKEN
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            settings.LLM_MODEL_NAME,
            device_map="auto",
            torch_dtype="auto",
            trust_remote_code=True
        )
        # Relie tokenizer/model au vectorizer
        self.vectorizer.tokenizer = self.tokenizer
        self.vectorizer.model = self.model

        # Initialise le graphe d‚Äôagent
        self.app = self._create_agent_graph()

    # === G√©n√©ration non-streaming (compl√®te) ===
    def _llm_generate(self, prompt: str, max_new_tokens=256, temperature=0.0) -> str:
        """G√©n√®re une r√©ponse compl√®te (sans streaming)"""
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        inputs = self.tokenizer(text, return_tensors="pt").to("cuda")

        with torch.no_grad():
            out_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=temperature > 0.0
            )[0]

        input_length = inputs.input_ids.shape[1]
        response_ids = out_ids[input_length:]
        return self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()

    # === Graphe RAG ===
    def _create_agent_graph(self):
        graph = StateGraph(RAGState)
        graph.add_node("agent_node", self.agent_node)
        graph.set_entry_point("agent_node")

        def decision_router(state):
            return "answer" if state.get("r√©ponse") else "terminate"

        graph.add_conditional_edges("agent_node", decision_router, {"answer": END, "terminate": END})
        return graph.compile()

    # === Outils ===
    def tool_reformulate(self, question: str, docs: list[dict]) -> str:
        context = "\n".join(f"- {d['text'][:500]}..." for d in docs[:3])
        prompt = (
            "Tu es un agent de la fonction publique du S√©n√©gal. "
            "La question suivante n‚Äôa pas obtenu de r√©ponse suffisante. "
            "Voici les extraits les plus pertinents trouv√©s :\n"
            f"{context}\n\n"
            "Reformule la question pour cibler plus pr√©cis√©ment l'information manquante, "
            "sans changer son sens.\n"
            f"Question originale : {question}\n"
            "Nouvelle question :"
        )
        return self._llm_generate(prompt, max_new_tokens=128, temperature=0.0)

    def tool_judge(self, question: str, docs: list[dict]) -> str:
        if not docs:
            return "REFORMULER"

        context = "\n\n".join(f"- {d['text']}" for d in docs)
        prompt = f"""Sur une √©chelle de 1-10, √† quel point ces extraits permettent-ils de r√©pondre √† la question ?
R√©ponds uniquement avec le chiffre.

Question: {question}

Extraits:
{context}

Score (1-10):"""

        score_text = self._llm_generate(prompt, max_new_tokens=3, temperature=0.0)
        try:
            score = int(score_text.strip())
            return "PERTINENT" if score >= 6 else "REFORMULER"
        except:
            return "REFORMULER"

    # === G√©n√©ration streaming ===
    async def _stream_answer(self, question: str, docs: list[dict]) -> AsyncGenerator[str, None]:
        """Streaming token-par-token via TextIteratorStreamer"""
        context = "\n\n".join(f"[{i+1}] {d['text']}" for i, d in enumerate(docs))
        prompt = (
            "R√©ponds √† la question en 5 phrases maximum.\n"
            "Inclue 1-2 citations EXACTES (entre guillemets) tir√©es des extraits.\n"
            "Ajoute une ligne 'Sources: ...' listant fichier:page.\n\n"
            f"Question:\n{question}\n\nExtraits:\n{context}\n\nR√©ponse:"
        )

        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        inputs = self.tokenizer(text, return_tensors="pt").to("cuda")

        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
        generation_kwargs = dict(
            **inputs,
            max_new_tokens=256,
            temperature=0.1,
            do_sample=True,
            streamer=streamer,
        )
        thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        # ‚ö° streamer consomm√© en direct ‚Üí streaming r√©el
        for new_text in streamer:
            yield new_text
            await asyncio.sleep(0)

        thread.join()

        # Ajout des sources
        sources = [f"{os.path.basename(d['source'])}:p.{d['page']+1}" for d in docs]
        yield f"\n\nüìö Sources: " + ", ".join(sorted(set(sources)))

    # === Agent node (logique RAG) ===
    def agent_node(self, state: RAGState):
        question = state["question"]

        for step in range(1, settings.MAX_STEPS + 1):
            docs = self.vectorizer.hybrid_search(question, top_k=settings.TOP_K)
            verdict = self.tool_judge(question, docs)

            if verdict == "PERTINENT":
                answer = self._llm_generate(
                    f"R√©ponds bri√®vement √† la question : {question}",
                    max_new_tokens=256,
                    temperature=0.1
                )
                return {
                    "decision": "answer",
                    "documents": [d["text"] for d in docs],
                    "r√©ponse": answer,
                    "question": question
                }
            else:
                if step == settings.MAX_STEPS:
                    return {
                        "decision": "terminate",
                        "documents": [d["text"] for d in docs],
                        "r√©ponse": "Je n‚Äôai trouv√© aucun document pertinent."
                    }
                question = self.tool_reformulate(question, docs)

        return {
            "decision": "terminate",
            "documents": [],
            "r√©ponse": "Arr√™t sans r√©ponse."
        }

    # === Interfaces ===
    async def generate_stream(self, question: str, max_tokens: int = 256, temperature: float = 0.1):
        """Streaming complet via agent_node"""
        for step in range(1, settings.MAX_STEPS + 1):
            docs = self.vectorizer.hybrid_search(question, top_k=settings.TOP_K)
            verdict = self.tool_judge(question, docs)

            if verdict == "PERTINENT":
                async for token in self._stream_answer(question, docs):
                    yield token
                return
            else:
                if step == settings.MAX_STEPS:
                    yield "\n‚ùå Aucun document pertinent trouv√©.\n"
                    return
                question = self.tool_reformulate(question, docs)

    def generate_sync(self, question: str) -> str:
        """Non-streaming (utilise agent_node via LangGraph)"""
        initial_state = {
            "question": question,
            "documents": [],
            "reformul√©": False,
            "reformulation_count": 0,
            "r√©ponse": None
        }
        final_state = self.app.invoke(initial_state)
        return final_state.get("r√©ponse", "Aucune r√©ponse g√©n√©r√©e.")
